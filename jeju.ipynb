{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 22:17:27.129249: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-30 22:17:27.145669: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-30 22:17:27.145689: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-30 22:17:27.146199: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 22:17:27.149209: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 22:17:27.581571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer, DataCollatorForSeq2Seq\n",
    "import json\n",
    "\n",
    "# GPU가 사용 가능한지 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  # 출력: 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/billy/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "/home/billy/anaconda3/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 토크나이저 설정\n",
    "MODEL_NAME = \"hyunwoongko/kobart\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. JSON 데이터 로드 및 전처리\n",
    "\n",
    "# JSON 파일 경로 설정\n",
    "json_file = \"jeju_data2.json\"  # JSON 파일 이름\n",
    "with open(json_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Dataset 객체로 변환\n",
    "dataset = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"source\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples[\"target\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f974d3e633d4408f9a5f9b08a789fb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196197 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 전처리\n",
    "processed_dataset = dataset.map(preprocess_function, batched=False)\n",
    "\n",
    "# 학습/평가 데이터 분리\n",
    "train_test_split = processed_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/home/billy/snap/geuneon/kobart-finetuned-jeju3\",  # 모델 출력 경로\n",
    "    evaluation_strategy=\"epoch\",  # 평가 전략 (매 epoch마다 평가)\n",
    "    save_strategy=\"epoch\",  # 체크포인트 저장 전략 (매 epoch마다 저장)\n",
    "    learning_rate=5e-5,  # 학습률\n",
    "    per_device_train_batch_size=8,  # 학습 시 배치 크기\n",
    "    per_device_eval_batch_size=8,  # 평가 시 배치 크기\n",
    "    num_train_epochs=3,  # 총 학습 epoch 수\n",
    "    weight_decay=0.01,  # weight decay\n",
    "    save_total_limit=2,  # 저장할 체크포인트의 최대 개수\n",
    "    logging_dir=\"/home/billy/snap/geuneon/logs3\",  # 로그 파일 저장 디렉토리\n",
    "    logging_steps=10000,  # 로그 작성 빈도 (step 단위)\n",
    "    report_to=\"none\",  # 외부 로깅 사용 안 함 (TensorBoard 등 제외)\n",
    "    load_best_model_at_end=True,  # 학습 종료 시 최상의 모델 로드\n",
    "    metric_for_best_model=\"eval_loss\",  # 최상의 모델 선택 기준\n",
    "    greater_is_better=False,  # 낮은 eval_loss일수록 좋은 모델\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66219' max='66219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66219/66219 53:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.054084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.052214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.053349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/billy/snap/geuneon/kobart-finetuned-jeju3/tokenizer_config.json',\n",
       " '/home/billy/snap/geuneon/kobart-finetuned-jeju3/special_tokens_map.json',\n",
       " '/home/billy/snap/geuneon/kobart-finetuned-jeju3/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 Collator 정의 (torch 관련 제거)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# 4. Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,  # DataCollatorForSeq2Seq를 사용\n",
    ")\n",
    "\n",
    "# 5. 모델 학습\n",
    "trainer.train()\n",
    "\n",
    "# 6. 학습된 모델 저장\n",
    "trainer.save_model(\"/home/billy/snap/geuneon/kobart-finetuned-jeju3\")\n",
    "tokenizer.save_pretrained(\"/home/billy/snap/geuneon/kobart-finetuned-jeju3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: 서껑 허는 건 뭐 , 보리ᄊᆞᆯ에 ᄑᆞᆺ도 서끄곡 , 콩도 서끄곡 .\n",
      "Generated Text: 섞어서 하는 건 뭐, 보리쌀에 팥도 섞고, 콩도 섞고.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# 저장된 모델 경로\n",
    "model_path = \"./kobart-finetuned-jeju3\"\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# 테스트 입력 데이터\n",
    "input_text = \"서껑 허는 건 뭐 , 보리ᄊᆞᆯ에 ᄑᆞᆺ도 서끄곡 , 콩도 서끄곡 .\"\n",
    "\n",
    "# 입력 텍스트를 토크나이즈\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# 모델 추론\n",
    "output_ids = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "# 생성된 텍스트 디코딩\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Input Text:\", input_text)\n",
    "print(\"Generated Text:\", output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Texts:\n",
      "어, 어, 그래. 어, 그렇게들 했지. 거 아주 좋은 거야. 참 범벅 절간고구마 해서 갈아서 범벅하면 참 맛있나. 그거 고급이라.\n",
      "갈아서 살ᄇᆞᆯ, ᄒᆞᆯᄆᆞᆯ.\n",
      "\n",
      "BLEU Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# 저장된 모델 경로\n",
    "model_path = \"./kobart-finetuned-jeju3\"\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# 테스트 입력 데이터 (여러 개의 문장으로 테스트)\n",
    "input_texts = [\n",
    "    \"ᄋᆞ , ᄋᆞ , 기여 . ᄋᆞ , 경덜 헤세 . 거 막 좋은 거여 . ᄎᆞᆷ 범벅 감제뻿데기 헹은에 ᄀᆞᆯ앙 범벅허민 ᄎᆞᆷ 맛좋나 . 그거 고급이라 .\",  # 원문\n",
    "    \"ᄀᆞᆯ앙은 ᄉᆞᆯᄇᆞᆯ ᄀᆞᆯᆞᆯ ᄒᆞᆯᄆᆞᆯ .\",  # 원문\n",
    "]\n",
    "\n",
    "# 참조 번역 (실제 번역, 각 문장의 번역 결과)\n",
    "reference_texts = [\n",
    "    [\"Ah, yes, contribution. Yes, it’s very good. The mixture of it is very tasty. That’s expensive.\"],  # 참조 번역\n",
    "    [\"The mixture is very delicious.\"],  # 참조 번역\n",
    "]\n",
    "\n",
    "# 입력 텍스트를 토크나이즈하고 모델에서 번역 생성\n",
    "generated_texts = []\n",
    "for input_text in input_texts:\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    output_ids = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    generated_texts.append(output_text)\n",
    "\n",
    "# BLEU 점수 계산\n",
    "# nltk의 corpus_bleu는 참조 번역과 생성된 번역을 비교하여 BLEU 점수를 계산\n",
    "bleu_score = corpus_bleu(reference_texts, generated_texts)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Generated Texts:\")\n",
    "for text in generated_texts:\n",
    "    print(text)\n",
    "\n",
    "print(f\"\\nBLEU Score: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
