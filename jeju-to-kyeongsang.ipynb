{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer, DataCollatorForSeq2Seq\n",
    "import json\n",
    "\n",
    "# GPU가 사용 가능한지 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  # 출력: 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 및 토크나이저 설정\n",
    "MODEL_NAME = \"hyunwoongko/kobart\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제주 JSON 데이터 로드 및 전처리\n",
    "\n",
    "# 제주 JSON 파일 경로 설정\n",
    "json_file = \"jeju_data.json\"  # JSON 파일 이름\n",
    "with open(json_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Dataset 객체로 변환\n",
    "dataset = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"source\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples[\"target\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리\n",
    "processed_dataset = dataset.map(preprocess_function, batched=False)\n",
    "\n",
    "# 학습/평가 데이터 분리\n",
    "train_test_split = processed_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제주 모델 파인튜닝\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./kobart-finetuned-jeju\",  # 모델 출력 경로\n",
    "    evaluation_strategy=\"epoch\",  # 평가 전략 (매 epoch마다 평가)\n",
    "    save_strategy=\"epoch\",  # 체크포인트 저장 전략 (매 epoch마다 저장)\n",
    "    learning_rate=5e-5,  # 학습률\n",
    "    per_device_train_batch_size=8,  # 학습 시 배치 크기\n",
    "    per_device_eval_batch_size=8,  # 평가 시 배치 크기\n",
    "    num_train_epochs=3,  # 총 학습 epoch 수\n",
    "    weight_decay=0.01,  # weight decay\n",
    "    save_total_limit=2,  # 저장할 체크포인트의 최대 개수\n",
    "    logging_dir=\"./jeju-logs\",  # 로그 파일 저장 디렉토리\n",
    "    logging_steps=10000,  # 로그 작성 빈도 (step 단위)\n",
    "    report_to=\"none\",  # 외부 로깅 사용 안 함 (TensorBoard 등 제외)\n",
    "    load_best_model_at_end=True,  # 학습 종료 시 최상의 모델 로드\n",
    "    metric_for_best_model=\"eval_loss\",  # 최상의 모델 선택 기준\n",
    "    greater_is_better=False,  # 낮은 eval_loss일수록 좋은 모델\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 Collator 정의 (torch 관련 제거)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# 4. Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,  # DataCollatorForSeq2Seq를 사용\n",
    ")\n",
    "\n",
    "# 5. 모델 학습\n",
    "trainer.train()\n",
    "\n",
    "# 6. 학습된 모델 저장\n",
    "trainer.save_model(\"./kobart-finetuned-jeju\")\n",
    "tokenizer.save_pretrained(\"./kobart-finetuned-jeju\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 경상 JSON 데이터 로드 및 전처리\n",
    "\n",
    "# 경상 JSON 파일 경로 설정\n",
    "json_file = \"kyeongsang_data.json\"  # JSON 파일 이름\n",
    "with open(json_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Dataset 객체로 변환\n",
    "dataset = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"source\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples[\"target\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리\n",
    "processed_dataset = dataset.map(preprocess_function, batched=False)\n",
    "\n",
    "# 학습/평가 데이터 분리\n",
    "train_test_split = processed_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#경상 모델 파인튜닝\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./kobart-finetuned-kyeongsang\",  # 모델 출력 경로\n",
    "    evaluation_strategy=\"epoch\",  # 평가 전략 (매 epoch마다 평가)\n",
    "    save_strategy=\"epoch\",  # 체크포인트 저장 전략 (매 epoch마다 저장)\n",
    "    learning_rate=5e-5,  # 학습률\n",
    "    per_device_train_batch_size=8,  # 학습 시 배치 크기\n",
    "    per_device_eval_batch_size=8,  # 평가 시 배치 크기\n",
    "    num_train_epochs=3,  # 총 학습 epoch 수\n",
    "    weight_decay=0.01,  # weight decay\n",
    "    save_total_limit=2,  # 저장할 체크포인트의 최대 개수\n",
    "    logging_dir=\"./kyeongsang-logs\",  # 로그 파일 저장 디렉토리\n",
    "    logging_steps=10000,  # 로그 작성 빈도 (step 단위)\n",
    "    report_to=\"none\",  # 외부 로깅 사용 안 함 (TensorBoard 등 제외)\n",
    "    load_best_model_at_end=True,  # 학습 종료 시 최상의 모델 로드\n",
    "    metric_for_best_model=\"eval_loss\",  # 최상의 모델 선택 기준\n",
    "    greater_is_better=False,  # 낮은 eval_loss일수록 좋은 모델\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 Collator 정의 (torch 관련 제거)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# 4. Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,  # DataCollatorForSeq2Seq를 사용\n",
    ")\n",
    "\n",
    "# 5. 모델 학습\n",
    "trainer.train()\n",
    "\n",
    "# 6. 학습된 모델 저장\n",
    "trainer.save_model(\"./kobart-finetuned-kyeongsang\")\n",
    "tokenizer.save_pretrained(\"./kobart-finetuned-kyeongsang\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# 저장된 모델 경로\n",
    "model_path = \"./kobart-finetuned-jeju\"\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# 테스트 입력 데이터\n",
    "input_text = \"서껑 허는 건 뭐 , 보리ᄊᆞᆯ에 ᄑᆞᆺ도 서끄곡 , 콩도 서끄곡 .\"\n",
    "\n",
    "# 입력 텍스트를 토크나이즈\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# 모델 추론\n",
    "output_ids = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "# 생성된 텍스트 디코딩\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Input Text:\", input_text)\n",
    "print(\"Generated Text:\", output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# 저장된 모델 경로\n",
    "model_path = \"./kobart-finetuned-kyeongsang\"\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# 테스트 입력 데이터\n",
    "input_text = \"그 의사 아 자기 들어봤어요 내가 들은 의사는 무엇이라 했냐면 잠들고 엄청 시간이 길지 않니 이게 완전히\"\n",
    "\n",
    "# 입력 텍스트를 토크나이즈\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# 모델 추론\n",
    "output_ids = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "# 생성된 텍스트 디코딩\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Input Text:\", input_text)\n",
    "print(\"Generated Text:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# 제주 방언 -> 표준어 모델 경로\n",
    "jeju_model_path = \"./kobart-finetuned-jeju3\"\n",
    "# 표준어 -> 경상도 방언 모델 경로\n",
    "gyeongsang_model_path = \"./kobart-finetuned-kyeongsang\"\n",
    "\n",
    "# 모델과 토크나이저 로드 (제주 방언 -> 표준어)\n",
    "jeju_model = BartForConditionalGeneration.from_pretrained(jeju_model_path)\n",
    "jeju_tokenizer = PreTrainedTokenizerFast.from_pretrained(jeju_model_path)\n",
    "\n",
    "# 모델과 토크나이저 로드 (표준어 -> 경상도 방언)\n",
    "gyeongsang_model = BartForConditionalGeneration.from_pretrained(gyeongsang_model_path)\n",
    "gyeongsang_tokenizer = PreTrainedTokenizerFast.from_pretrained(gyeongsang_model_path)\n",
    "\n",
    "# 테스트 입력 데이터 (제주 방언)\n",
    "input_text_jeju = \"서껑 허는 건 뭐 , 보리ᄊᆞᆯ에 ᄑᆞᆺ도 서끄곡 , 콩도 서끄곡 .\"\n",
    "\n",
    "# 입력 텍스트 출력\n",
    "print(\"Input Text (Jeju Dialect):\", input_text_jeju)\n",
    "\n",
    "# 1단계: 제주 방언 -> 표준어 번역\n",
    "input_ids_jeju = jeju_tokenizer.encode(input_text_jeju, return_tensors=\"pt\")\n",
    "output_ids_jeju = jeju_model.generate(input_ids_jeju, max_length=50, num_beams=4, early_stopping=True)\n",
    "output_text_jeju = jeju_tokenizer.decode(output_ids_jeju[0], skip_special_tokens=True)\n",
    "\n",
    "# 표준어 결과 출력\n",
    "print(\"Standard Language:\", output_text_jeju)\n",
    "\n",
    "# 2단계: 표준어 -> 경상도 방언 번역\n",
    "input_ids_standard = gyeongsang_tokenizer.encode(output_text_jeju, return_tensors=\"pt\")\n",
    "output_ids_gyeongsang = gyeongsang_model.generate(input_ids_standard, max_length=50, num_beams=4, early_stopping=True)\n",
    "output_text_gyeongsang = gyeongsang_tokenizer.decode(output_ids_gyeongsang[0], skip_special_tokens=True)\n",
    "\n",
    "# 경상도 방언 결과 출력\n",
    "print(\"Gyeongsang Dialect:\", output_text_gyeongsang)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
